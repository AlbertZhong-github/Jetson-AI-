{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jetson Nano for Visual Recognition\n",
    "### Using the ssd_mobilenet_v2_coco vision model on Jetson Nano to achieve object recognition in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p>\n",
    " <span style=\"color: green;\"> The method for running the program:<br>\n",
    "   This program is intended to run on a Jetson Nano environment. To enable the JetBot car to move freely, the Jetson Nano system board must not be connected to power cables, mice, monitors, or other USB cables. Instead, access and execution must be done through a web-based Jupyter lab accessed remotely over the network. The IP address of the Jetson Nano, displayed on its LCD screen when connected to the local network, will be in the format of 192.168.1.XXX (where XXX is a two- or three-digit number).<br>\n",
    "   Steps to initiate the program:<br><br>\n",
    " 1. Insert an SD card with the JetPack software image burned onto it.<br>\n",
    " 2. Connect a monitor to the HDMI port of the Jetson Nano, and plug in a keyboard and mouse into the USB ports. Power on the Jetson Nano system.<br>\n",
    " 3. Navigate to Ubuntu 18.04, and configure the local network connection settings for the Jetson Nano system as shown in Figure-1.<br>\n",
    " 4. Disconnect the monitor, keyboard, mouse, USB devices, and power cable. Restart the Jetson Nano. At this point, the local IP address of the Jetson Nano will be visible on the LCD display of the smart car, as shown in Figure-2.<br>\n",
    " 5. On an external computer connected to the same local network, enter the local IP address of the Jetson Nano into a web browser to establish a connection to the Jetson Nano system, as illustrated in Figure-3.<br>\n",
    " <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <img class=\" long-press-able-img \" src=\"Ubuntu1.jpg\" style=\"width: 30%; height: auto; margin-right: 1%;\">\n",
    "    <img class=\" long-press-able-img \" srce-img_q1nu4_69 long-press-able-img \" src=\"液晶1.jpg\" style=\"width: 30%; height: auto; margin-right: 1%;\">\n",
    "    <img class=\"_double-img_q1nu4_69 long-press-able-img \" src=\"远程1.jpg\" style=\"width: 30%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the Topic 2 of the VAIC (Vision-Aided Intelligent Challenge) technical notes, we delve into leveraging the Jetson Nano platform to perform object recognition using visual models. In the context of VAIC competitions, often the focus is on identifying a limited set of objects (e.g., VAIC_23_24 involves distinguishing between three differently colored balls of the same shape, and VAIC_24_25 involves rings with similar shapes but varying colors).<br>\n",
    "Adopting pre-existing, powerful, and reliable multi-object vision models and fine-tuning them specifically for the target objects of the competition is a straightforward and rapid approach to implementing the vision capabilities of VAIC robots. These robust, multi-purpose vision models, known as pretrained models, have been trained on vast amounts of data and can be utilized to address similar or related problems without the need for training from scratch. Pretrained models significantly reduce the training time and data requirements for new tasks, enhancing the model's generalization ability and accuracy. In fields such as Natural Language Processing (NLP) and Computer Vision (CV), the use of pretrained models has become a standard practice.<br>\n",
    "Therefore, this topic emphasizes introducing algorithms and program implementations on the Jetson Nano for recognizing various objects in the surrounding environment using existing multi-object vision models. This aims to equip AI beginners with an understanding of the general workings of vision models and the implementation of visual reasoning algorithms on the Jetson Nano. For enhancing the recognition performance of specific targets in VAIC competitions through fine-tuning pretrained models, detailed instructions will be covered in subsequent topics.<br>\n",
    "Note: For information on the Jetson Nano system, the camera used, and the vehicle that carries the Jetson Nano, please refer to the \"Introduction_to_Jetson-Nano_of_system.ipynb\" document located within this directory.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘from jetbot import ObjectDetector’\n",
    "imports the ObjectDetector module from the NVIDIA jetbot function library. jetbot is a Python library developed for the NVIDIA Jetson Nano, designed to simplify the development of robotic projects such as autonomous vehicles, surveillance robots, and more. This library provides a range of tools and modules that assist developers in rapidly building and deploying projects based on the Jetson Nano. Among the jetbot library, ObjectDetector is a particularly crucial component, as it is used for detecting objects within images.<br>\n",
    "\n",
    "‘model = ObjectDetector('ssd_mobilenet_v2_coco.engine')’<br>\n",
    "imports the ssd_mobilenet_v2_coco vision model. The ssd_mobilenet_v2_coco is a deep learning model commonly used for object detection tasks. It combines the earlier MobileNet v2 model with the later SSD model and is trained on the COCO dataset.<br>\n",
    "A brief explanation is as follows:<br>\n",
    "1.SSD (Single Shot MultiBox Detector):<br>\n",
    "Single Shot: This model completes the object detection task in a single forward pass, making it very fast.<br>\n",
    "MultiBox: Refers to the generation and processing of multiple candidate bounding boxes in SSD.<br>\n",
    "Detector: The primary function of SSD is to perform object detection, i.e., identifying objects in images, classifying them into predefined categories, and outputting their locations (bounding boxes).<br>\n",
    "MobileNet V2:<br>\n",
    "2.MobileNet V2 is a lightweight and efficient convolutional neural network (CNN) architecture designed specifically for mobile devices and embedded vision applications. It employs depthwise separable convolutions, significantly reducing the number of parameters and computations compared to standard convolutions, making it ideal for use in resource-constrained environments.<br>\n",
    "3.COCO Dataset:<br>\n",
    "COCO (Common Objects in Context) is a large-scale image dataset containing hundreds of thousands of images across 80 object categories. It is widely used for tasks such as object detection, semantic segmentation, and keypoint detection. The SSD MobileNet V2 COCO model is typically trained on this dataset, enabling it to detect common objects like people, cars, animals, etc.<br>\n",
    "For a detailed explanation of the ssd_mobilenet_v2_coco model's network structure and operational principles, please refer to the appendix titled \"Technical _Specification_of_SSD_MobileNet_V2_COCO_Model.ipynb\" Interested readers may consult this document for further information.<br>\n",
    "The .engine suffix indicates that this model has been optimized using TensorRT, enabling high-speed inference on NVIDIA Jetson series GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 毛毛熊\n"
     ]
    }
   ],
   "source": [
    "def parse_data(file_path):\n",
    "    items = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        item = {}\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line == \"item {\":  # Skip empty lines and start of a new item\n",
    "                continue\n",
    "            if line == \"}\":  # End of current item\n",
    "                items.append(item)\n",
    "                item = {}\n",
    "            else:\n",
    "                try:\n",
    "                    key, value = line.split(\": \", 1)  # Only split on the first colon\n",
    "                    key = key.strip()\n",
    "                    value = value.strip().strip('\"')\n",
    "                    if key == \"name\" and key in item:  # Handle duplicate 'name' keys\n",
    "                        item[\"zh_name\"] = value\n",
    "                    else:\n",
    "                        item[key] = value\n",
    "                except ValueError as e:  # Catch lines that don't have a colon\n",
    "                    print(f\"Error processing line: {line} - {e}\")\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "file_path = 'recognize_objects.json'\n",
    "data = parse_data(file_path)\n",
    "#key='zh_name'\n",
    "#id=88\n",
    "#print(data[id]['id'],data[88][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To read out the 80 recognized object category labels and display names for the ssd_mobilenet_v2_coco model when identifying objects from the COCO dataset, the data file is named \"识别物体.json\" located in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "w=300\n",
    "h=300\n",
    "camera = Camera.instance(width=w, height=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Import the camera function module from the jetbot Python library, set the image dimensions, and create a Camera instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "#print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the captured image (camera.value) from the camera (camera) to the model (model) for recognition and store the detected objects or features in a variable named detections, following the format described:<br>\n",
    "key=\"detection_boxes\": list(int) #Bounding boxes for objects  <br>\n",
    "key=\"detection_classes\"：list(int) # List of class indices  <br>\n",
    "key=\"detection_scores\":list(float) # Confidence scores for each detection <br>\n",
    "key=\"num_detections\": int # Total number of detected objects  <br>\n",
    "For example:<br>\n",
    "{  <br>\n",
    "  \"detection_boxes\": [  <br>\n",
    "    [0.1, 0.2, 0.5, 0.6],  // Bounding box for the first object (xmin, ymin, xmax, ymax)  <br>\n",
    "    [0.4, 0.4, 0.7, 0.8],  // Bounding box for the second object  <br>\n",
    "    [0.3, 0.3, 0.6, 0.7]   // Bounding box for the third object  <br>\n",
    "  ],  <br>\n",
    "  \"detection_classes\": [1, 3, 17],  // Detected class indices (e.g., 1 represents person, 3 represents car, 17 represents dog)  <br>\n",
    "  \"detection_scores\": [0.95, 0.89, 0.78],  // Confidence scores for each detection  <br>\n",
    "  \"num_detections\": 3  // Total number of detected objects  <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "\n",
    "detections_widget.value = str(detections)\n",
    "\n",
    "#display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook or other interactive Python environments for creating and displaying interactive widgets, follow <br>\n",
    "\n",
    "1.from IPython.display import display<br>\n",
    "This imports the display function from the IPython.display module. The display function is used to render or show widgets or other output content within Jupyter Notebook.<br>\n",
    "\n",
    "2.import ipywidgets.widgets as widgets<br>\n",
    "This imports the widgets module from the ipywidgets library. ipywidgets is a package for creating interactive HTML widgets in Jupyter Notebook.<br>\n",
    "\n",
    "3.detections_widget = widgets.Textarea()<br>\n",
    "This creates a Textarea widget and assigns it to the variable detections_widget. A Textarea is a multiline text box that allows users to input or display text. In this context, it's being used to display the contents of the detections variable.<br>\n",
    "\n",
    "4.detections_widget.value = str(detections)<br>\n",
    "This sets the content of the Textarea widget to the string representation of the detections variable. By using str(detections), the detections variable is converted into a string, which is then displayed as text within the Textarea.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "#print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize image_number and object_number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from jetbot import Robot:<br>\n",
    "Imports the Robot class from the jetbot package. The jetbot package is specifically designed for the JetBot robotics platform, encompassing a range of classes and functions for controlling the robot, processing camera inputs, conducting image recognition, and more.<br>\n",
    "robot = Robot():<br>\n",
    "Creates an instance of the Robot class and assigns it to the variable robot. Through this instance, you can access all the methods and attributes defined within the Robot class, enabling you to control the behavior of the JetBot robot. The constructor of Robot() is responsible for initializing the hardware interfaces required by the robot (such as motors, cameras, etc.) and setting up some basic configurations.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "import cv2\n",
    "import numpy as np\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc,20.0,(300,300))\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.5\n",
    "text_color = (255,255,255)\n",
    "thickness = 2\n",
    "#blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = widgets.Image(format='jpeg', width=w, height=h)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget]),#, blocked_widget]),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above text defines ipywidgets widgets for various purposes:<br>\n",
    "image_widget for displaying images<br>\n",
    "label_widget for displaying labels<br>\n",
    "speed_widget for controlling the movement speed of a vehicle<br>\n",
    "turn_gain_widget for controlling the turning speed or gain of a vehicle<br>\n",
    "In Jupyter Notebook or JupyterLab, the display function in conjunction with the ipywidgets library can be used to create and display interactive widgets. The display(widgets.VBox([...])) call demonstrates how to organize widget layouts using VBox (Vertical Box) and HBox (Horizontal Box).<br>\n",
    "display function: This function is specific to Jupyter Notebook and JupyterLab and is used to display objects in the output cell. Here, it is utilized to show a VBox widget that contains other widgets.<br>\n",
    "widgets.VBox: VBox is a class in the ipywidgets library that creates a container for vertical layout. It takes a list as an argument, which contains the widgets to be arranged vertically.<br>\n",
    "widgets.HBox([image_widget]): Here, an HBox (Horizontal Box) is created, which is a container for arranging widgets horizontally. However, in this specific example, the HBox only contains a single widget—the image_widget.<br>\n",
    "label_widget, speed_widget, turn_gain_widget: These are already created widgets used for displaying labels, controlling speed, and adjusting turning gain, respectively. <br>\n",
    "They are directly added to the VBox, resulting in a vertical arrangement.<br>\n",
    "Layout: Ultimately, the VBox container vertically arranges the widgets in the following order: first, an HBox containing the image_widget (though in this particular case, the HBox only holds one element), followed by label_widget, speed_widget, and turn_gain_widget.<br>\n",
    "When the display function is called, it renders this VBox container in the output cell of Jupyter Notebook or JupyterLab, allowing users to see the vertically arranged widget layout.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rgb_to_jpeg(rgb_image):\n",
    "\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),90] #设置JPEG的质量\n",
    "    result,encoded_img = cv2.imencode('.jpg',rgb_image,encode_param)\n",
    "    if result:\n",
    "        return encoded_img.tobytes()\n",
    "    else:\n",
    "        raise ValueError(\"Image encoding failed\")\n",
    "        \n",
    "        \n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1.Define the rgb_to_jpeg function that accepts an RGB image (a NumPy array) as input and returns its JPEG-encoded byte stream.<br>\n",
    "To set the JPEG quality:<br>\n",
    "cv2.IMWRITE_JPEG_QUALITY is a flag defined in OpenCV used to specify the quality when saving JPEG images. 90 is the quality value, ranging from 0 (worst quality, smallest file size) to 100.<br>\n",
    "Encoding the image:<br>\n",
    "Use the cv2.imencode function to encode the image into JPEG format. This function expects the image data to be in the format OpenCV expects, which is BGR order instead of RGB. Therefore, if your input image is in RGB format, you need to convert it to BGR first.<br>\n",
    "Handling the result:<br>\n",
    "cv2.imencode returns a boolean value and the encoded image (if successful). The boolean indicates whether the operation was successful, and the second return value is the encoded image data (if the operation was successful).<br>\n",
    "2.detection_center(detection) function<br>\n",
    "This function takes a detection object (detection) as input, which should include a key 'bbox', whose value is a list or tuple containing four elements representing the coordinates of the bounding box. These four elements are typically defined as (x_min, y_min, width, height), i.e., the x and y coordinates of the top-left corner of the bounding box, along with its width and height.<br>\n",
    "The goal of the function is to calculate the x and y coordinates of the center of this bounding box. However, there's a notable point: after calculating the center coordinates, it subtracts 0.5 from each. This could be to adjust the coordinate system to a specific one, possibly centered on the image's center, but it's not the standard way to calculate the center of a bounding box. The standard approach only involves using (x_min + width / 2, y_min + height / 2) as the center coordinates.<br>\n",
    "3.norm(vec) function<br>\n",
    "This function takes a 2D vector (vec) as input and returns the length (or Euclidean distance, magnitude, or norm) of the vector. This is achieved by calculating the square root of the sum of squares of the vector's components, i.e., np.sqrt(vec[0]**2 + vec[1]**2). This function is used to calculate the distance from the center of a detection object to the center of the image (or an assumed origin) in subsequent computations.<br>\n",
    "4.closest_detection(detections) function<br>\n",
    "This function takes a list of detection objects (detections) as input and aims to find which detection object in this list has a center closest to the center of the image (or the assumed origin, if the coordinate adjustment in the detection_center function was intended for this purpose).\n",
    "The function iterates through each detection object, calculates its center coordinates, and computes the distance from this center to the origin (or image center). By comparing these distances, the function ultimately returns the detection object with the center closest to the origin.<br>\n",
    "Note: If the detections list is empty, or if all detection objects' centers lie on a circle equidistant from the origin, the function will return the first detection object in the list as the \"closest\" one, as it doesn't handle priority conflicts in such cases.<br>\n",
    "Additionally, due to the coordinate adjustment (subtracting 0.5) in the detection_center function, this \"closest\" calculation is actually based on a specific, potentially non-standard coordinate system. If this is not the intended behavior, the detection_center function might need adjustment to return the standard bounding box center coordinates.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"\n",
    "zh_font = ImageFont.truetype(font_path,13)        \n",
    "def execute(change):\n",
    "    labels=[]\n",
    "    image = change['new']\n",
    "   \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "\n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        labelv = det['label']\n",
    "        text = data[labelv]['display_name']\n",
    "        #labels.append(det['id'])\n",
    "        #print(data[labelv][key])\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), \n",
    "                                                                      int(height * bbox[3])), (0, 0, 0), 1)\n",
    "        cv2.putText(image,text,(int(width * bbox[0]),int(height * bbox[1])),font,font_scale,text_color,thickness)\n",
    "        '''\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(frame_pil)\n",
    "        draw.text((int(width * bbox[0]),int(height * bbox[1])),text,font=zh_font,fill=(255,255,255,1))\n",
    "        frame_with_text = cv2.cvtColor(np.array(frame_pil),cv2.COLOR_RGB2BGR)\n",
    "        '''\n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    \n",
    "   \n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        #cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), \n",
    "        #int(height * bbox[3])), (0,0,0), 1)\n",
    "        \n",
    "    rgb_image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image_widget.value = rgb_to_jpeg(image)\n",
    "    out.write(rgb_image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This piece of code represents a function that processes image detection results, utilizing OpenCV (via cv2) and PIL (Python Imaging Library, potentially through PIL.Image or similar modules for image manipulations) to draw bounding boxes and labels for detected objects on an image, and handle specific detection outcomes.<br>\n",
    "execute(change)<br>\n",
    "Purpose: Receives a new image change, processes it, and draws bounding boxes and labels for detected objects on the image.<br>\n",
    "Parameters: change is a dictionary where the key 'new' corresponds to an image (in the form of a NumPy array or another format convertible to an image).<br>\n",
    "Process:<br>\n",
    "Initialization: Creates an empty list labels (though not used later in the described code), and retrieves the image corresponding to the 'new' key in the change dictionary.<br>\n",
    "Object Detection: Invokes a function named model (defined elsewhere in the code) to process the image and return detection results. The model is mentioned as 'ssd_mobilenet_v2_coco.engine', indicating it's an image detection model.<br>\n",
    "Drawing Detection Results:<br>\n",
    "Iterates through each object in the detection results.<br>\n",
    "Uses the bounding box (bbox) and label (label) to draw a rectangle and text label.<br>\n",
    "Utilizes OpenCV's cv2.rectangle and cv2.putText functions. Note that width and height refer to the image's dimensions.<br>\n",
    "Filtering Results by Specific Categories:<br>\n",
    "Filters detection results to match a specific category based on label_widget.value (not defined in the provided code snippet, likely a value from a UI component).<br>\n",
    "Finding the Closest Detection to the Center:<br>\n",
    "Assumes closest_detection is an undefined function that finds the detection result whose center is closest to the center of the image, among the filtered results.<br>\n",
    "Handling a Specific Detection Result (if found):<br>\n",
    "Draws the bounding box for this detection (though the drawing code is commented out).<br>\n",
    "Converting and Displaying the Image:<br>\n",
    "Converts the image from BGR format to RGB format.<br>\n",
    "image_widget.value is a UI component used to display the processed image. Here, the rgb_to_jpeg function (as defined earlier) is utilized to convert the image to a JPEG-encoded byte stream.<br>\n",
    "out is mentioned as a video file writer or similar output stream used to save the processed image, but its usage or definition is not detailed in the context provided.<br>\n",
    "Note: The described process assumes familiarity with OpenCV's functionality for image processing and drawing, as well as an understanding of how the detection model (model) and potential UI components (label_widget, image_widget) work within the larger context of the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"600\" height=\"400\" controls>\n",
    "  <source src=\"/Users/lichengtong/Jetson旋转式物体识别/识别结果.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.left(speed=0.075)\n",
    "robot.right(speed=-0.075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the rotation speed of the car and start rotating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the block below to connect the execute function to each camera frame update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"600\" height=\"400\" controls>\n",
    "  <source src=\"/Users/lichengtong/Jetson旋转式物体识别/识别现场.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "camera.unobserve_all()<br>\n",
    "Removes all previously bound callbacks from the camera object. In the Widgets library, when you want to perform certain actions whenever the value of a Widget changes, you use the .observe() method to bind a callback function (such as the execute function) to that Widget. If later you no longer need these callback functions, or want to rebind new callback functions, you can use .unobserve_all() to remove all existing callbacks, avoiding unnecessary executions or potential errors.<br>\n",
    "camera.observe(execute, names='value')<br>\n",
    "Binds the execute function as an observer to the camera object, so that the execute function is automatically called whenever the value attribute of the camera object changes. Here, camera is the Widget for capturing images, and the value attribute represents the currently captured image or video frame.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "out.release()\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out.release() releases the video output and closes the video file.<br>\n",
    "camera.unobserve_all()<br>\n",
    "Removes all previously bound callbacks from the camera object. In the Widgets library, when you want to perform certain actions whenever the value of a Widget changes, you use the .observe() method to bind a callback function (such as the execute function) to that Widget. If later you no longer need these callback functions, or want to rebind new callback functions, you can use .unobserve_all() to remove all existing callbacks, avoiding unnecessary executions or potential errors.<br>\n",
    "camera.observe(execute, names='value')<br>\n",
    "Binds the execute function as an observer to the camera object, so that the execute function is automatically called whenever the value attribute of the camera object changes. Here, camera is the Widget for capturing images, and the value attribute represents the currently captured image or video frame.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
